# Cloudflare ä¸Šä¼ é€Ÿåº¦ä¼˜åŒ–æŒ‡å—

## ğŸ“Š å½“å‰ä¸Šä¼ æ¶æ„åˆ†æ

### ç°æœ‰æµç¨‹
1. **å®¢æˆ·ç«¯** â†’ FormDataå¤šæ–‡ä»¶ â†’ **Workers**
2. **Workers** â†’ è¯»å–æ–‡ä»¶ â†’ **R2å­˜å‚¨**
3. å•æ¬¡è¯·æ±‚å®Œæ•´ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶

### æ€§èƒ½ç“¶é¢ˆ
- âŒ Workerså•æ¬¡è¯·æ±‚å¤§å°é™åˆ¶ï¼š100MB
- âŒ å¤§æ–‡ä»¶éœ€å®Œæ•´ä¸Šä¼ åæ‰èƒ½å¼€å§‹å¤„ç†
- âŒ æ— å¹¶è¡Œä¸Šä¼ èƒ½åŠ›
- âŒ ç½‘ç»œä¸­æ–­éœ€è¦é‡æ–°å¼€å§‹

## ğŸš€ ä¼˜åŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šå®¢æˆ·ç«¯åˆ†å—ä¸Šä¼ ï¼ˆæ¨èï¼‰â­

**åŸç†**: åœ¨å®¢æˆ·ç«¯å°†å¤§æ–‡ä»¶åˆ†æˆå¤šä¸ªå°å—ï¼Œå¹¶è¡Œä¸Šä¼ åˆ°Workers

**ä¼˜åŠ¿**:
- âœ… çªç ´Workerså•æ¬¡è¯·æ±‚100MBé™åˆ¶
- âœ… æ”¯æŒå¹¶è¡Œä¸Šä¼ ï¼ˆæœ€é«˜8-16ä¸ªå¹¶å‘ï¼‰
- âœ… æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- âœ… æ›´å¥½çš„è¿›åº¦æ˜¾ç¤º
- âœ… ç½‘ç»œå®¹é”™æ€§å¼º

**å®ç°æ­¥éª¤**:

#### 1. å®¢æˆ·ç«¯åˆ†å—ä»£ç 

```javascript
// é…ç½®
const CHUNK_SIZE = 5 * 1024 * 1024;  // 5MB per chunk
const MAX_CONCURRENT = 8;            // æœ€å¤§å¹¶å‘æ•°

async function uploadFileInChunks(file, password) {
  const totalChunks = Math.ceil(file.size / CHUNK_SIZE);
  const uploadId = generateUploadId();

  // 1. åˆå§‹åŒ–ä¸Šä¼ 
  const initResponse = await fetch('/api/upload/init', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      uploadId,
      fileName: file.name,
      fileSize: file.size,
      totalChunks,
      password
    })
  });

  // 2. åˆ†å—å¹¶è¡Œä¸Šä¼ 
  const chunks = [];
  for (let i = 0; i < totalChunks; i++) {
    const start = i * CHUNK_SIZE;
    const end = Math.min(start + CHUNK_SIZE, file.size);
    chunks.push({ index: i, start, end });
  }

  let uploadedChunks = 0;
  const uploadChunk = async (chunk) => {
    const blob = file.slice(chunk.start, chunk.end);
    const formData = new FormData();
    formData.append('uploadId', uploadId);
    formData.append('chunkIndex', chunk.index);
    formData.append('chunk', blob);

    const response = await fetch('/api/upload/chunk', {
      method: 'POST',
      body: formData
    });

    if (response.ok) {
      uploadedChunks++;
      const progress = (uploadedChunks / totalChunks) * 90; // ä¸Šä¼ å 90%
      updateProgress(progress);
    }
    return response;
  };

  // ä½¿ç”¨Promiseå¹¶å‘æ§åˆ¶ä¸Šä¼ 
  await uploadInBatches(chunks, uploadChunk, MAX_CONCURRENT);

  // 3. å®Œæˆä¸Šä¼ å¹¶è§¦å‘å‹ç¼©
  const completeResponse = await fetch('/api/upload/complete', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ uploadId })
  });

  return completeResponse.json();
}

// å¹¶å‘æ§åˆ¶å‡½æ•°
async function uploadInBatches(items, handler, concurrency) {
  const results = [];
  for (let i = 0; i < items.length; i += concurrency) {
    const batch = items.slice(i, i + concurrency);
    const batchResults = await Promise.all(batch.map(handler));
    results.push(...batchResults);
  }
  return results;
}
```

#### 2. æœåŠ¡å™¨ç«¯æ¥å£å®ç°

```javascript
// åˆå§‹åŒ–ä¸Šä¼ 
async function handleUploadInit(request, env) {
  const { uploadId, fileName, fileSize, totalChunks, password } = await request.json();

  const hashedPwd = await hashPassword(password);

  const uploadMeta = {
    uploadId,
    fileName,
    fileSize,
    totalChunks,
    password: hashedPwd,
    chunks: new Array(totalChunks).fill(false),
    createdAt: Date.now(),
    status: 'uploading'
  };

  await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(uploadMeta));

  return jsonResponse({ success: true, uploadId });
}

// æ¥æ”¶åˆ†å—
async function handleUploadChunk(request, env) {
  const formData = await request.formData();
  const uploadId = formData.get('uploadId');
  const chunkIndex = parseInt(formData.get('chunkIndex'));
  const chunk = formData.get('chunk');

  // å­˜å‚¨åˆ†å—åˆ°R2
  const chunkKey = `temp/${uploadId}/chunk-${chunkIndex}`;
  await env.FILE_STORAGE.put(chunkKey, chunk);

  // æ›´æ–°ä¸Šä¼ çŠ¶æ€
  const metaStr = await env.FILE_META.get(`upload:${uploadId}`);
  const meta = JSON.parse(metaStr);
  meta.chunks[chunkIndex] = true;
  await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(meta));

  return jsonResponse({
    success: true,
    uploaded: meta.chunks.filter(Boolean).length,
    total: meta.totalChunks
  });
}

// å®Œæˆä¸Šä¼ 
async function handleUploadComplete(request, env, ctx) {
  const { uploadId } = await request.json();

  // è·å–ä¸Šä¼ å…ƒæ•°æ®
  const metaStr = await env.FILE_META.get(`upload:${uploadId}`);
  const meta = JSON.parse(metaStr);

  // éªŒè¯æ‰€æœ‰åˆ†å—å·²ä¸Šä¼ 
  if (meta.chunks.some(c => !c)) {
    return errorResponse('éƒ¨åˆ†åˆ†å—æœªä¸Šä¼ å®Œæˆ');
  }

  // è§¦å‘åˆå¹¶å’Œå‹ç¼©
  ctx.waitUntil(mergeAndCompress(uploadId, meta, env));

  return jsonResponse({ success: true, uploadId });
}

// åˆå¹¶åˆ†å—å¹¶å‹ç¼©
async function mergeAndCompress(uploadId, meta, env) {
  try {
    // 1. è¯»å–æ‰€æœ‰åˆ†å—
    const chunks = [];
    for (let i = 0; i < meta.totalChunks; i++) {
      const chunkKey = `temp/${uploadId}/chunk-${i}`;
      const obj = await env.FILE_STORAGE.get(chunkKey);
      chunks.push(await obj.arrayBuffer());
    }

    // 2. åˆå¹¶ä¸ºå®Œæ•´æ–‡ä»¶
    const fileData = new Uint8Array(meta.fileSize);
    let offset = 0;
    for (const chunk of chunks) {
      fileData.set(new Uint8Array(chunk), offset);
      offset += chunk.byteLength;
    }

    // 3. å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
    const filesToZip = { [meta.fileName]: fileData };
    const zipped = zipSync(filesToZip, { level: 3 });

    // 4. å­˜å‚¨æœ€ç»ˆæ–‡ä»¶
    const fileId = generateFileId();
    await env.FILE_STORAGE.put(fileId, zipped);

    // 5. ä¿å­˜å…ƒæ•°æ®
    const finalMeta = {
      fileId,
      password: meta.password,
      expiryTime: getExpiryTime(),
      createdAt: Date.now(),
      fileName: 'files.zip',
      fileSize: zipped.byteLength,
    };
    await env.FILE_META.put(fileId, JSON.stringify(finalMeta));

    // 6. åˆ é™¤ä¸´æ—¶åˆ†å—
    for (let i = 0; i < meta.totalChunks; i++) {
      await env.FILE_STORAGE.delete(`temp/${uploadId}/chunk-${i}`);
    }

    // 7. æ›´æ–°ä¸Šä¼ çŠ¶æ€
    meta.status = 'completed';
    meta.fileId = fileId;
    await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(meta));

  } catch (error) {
    console.error('Merge error:', error);
    meta.status = 'failed';
    meta.error = error.message;
    await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(meta));
  }
}
```

**é¢„æœŸæ€§èƒ½æå‡**:
- 10GBæ–‡ä»¶ï¼šä»çº¦10åˆ†é’Ÿ â†’ **2-3åˆ†é’Ÿ**
- 1GBæ–‡ä»¶ï¼šä»çº¦1åˆ†é’Ÿ â†’ **15-20ç§’**

---

### æ–¹æ¡ˆ2ï¼šR2 Multipart Uploadï¼ˆé«˜çº§ï¼‰

**åŸç†**: ç›´æ¥ä½¿ç”¨R2çš„S3å…¼å®¹Multipart Upload API

**ä¼˜åŠ¿**:
- âœ… åŸç”ŸR2æ”¯æŒï¼Œæ€§èƒ½æœ€ä¼˜
- âœ… å¯è¾¾åˆ° **1600+ MB/s** ä¸Šä¼ é€Ÿåº¦
- âœ… æ”¯æŒæœ€å¤§5TBå•æ–‡ä»¶
- âœ… è‡ªåŠ¨ç®¡ç†åˆ†å—

**é™åˆ¶**:
- âš ï¸ æ¯ä¸ªåˆ†å—æœ€å°5MBï¼ˆæœ€åä¸€å—é™¤å¤–ï¼‰
- âš ï¸ æœ€å¤§10,000ä¸ªåˆ†å—
- âš ï¸ æ‰€æœ‰åˆ†å—ï¼ˆé™¤æœ€åä¸€å—ï¼‰å¿…é¡»ç›¸åŒå¤§å°

**å®ç°ç¤ºä¾‹**:

```javascript
import { S3Client, CreateMultipartUploadCommand, UploadPartCommand, CompleteMultipartUploadCommand } from "@aws-sdk/client-s3";

// åˆå§‹åŒ–S3å®¢æˆ·ç«¯ï¼ˆç”¨äºR2ï¼‰
const s3Client = new S3Client({
  region: "auto",
  endpoint: `https://${ACCOUNT_ID}.r2.cloudflarestorage.com`,
  credentials: {
    accessKeyId: R2_ACCESS_KEY_ID,
    secretAccessKey: R2_SECRET_ACCESS_KEY,
  },
});

async function uploadToR2Multipart(file, key) {
  const PART_SIZE = 10 * 1024 * 1024; // 10MB per part

  // 1. åˆ›å»ºmultipart upload
  const createCommand = new CreateMultipartUploadCommand({
    Bucket: "fastfile-storage",
    Key: key,
  });
  const { UploadId } = await s3Client.send(createCommand);

  // 2. ä¸Šä¼ æ‰€æœ‰åˆ†å—
  const parts = [];
  const totalParts = Math.ceil(file.size / PART_SIZE);

  for (let i = 0; i < totalParts; i++) {
    const start = i * PART_SIZE;
    const end = Math.min(start + PART_SIZE, file.size);
    const partData = file.slice(start, end);

    const uploadCommand = new UploadPartCommand({
      Bucket: "fastfile-storage",
      Key: key,
      UploadId,
      PartNumber: i + 1,
      Body: await partData.arrayBuffer(),
    });

    const { ETag } = await s3Client.send(uploadCommand);
    parts.push({ PartNumber: i + 1, ETag });
  }

  // 3. å®Œæˆmultipart upload
  const completeCommand = new CompleteMultipartUploadCommand({
    Bucket: "fastfile-storage",
    Key: key,
    UploadId,
    MultipartUpload: { Parts: parts },
  });

  return await s3Client.send(completeCommand);
}
```

**æ³¨æ„**: éœ€è¦é…ç½®R2 API tokenså¹¶å®‰è£… `@aws-sdk/client-s3`

---

### æ–¹æ¡ˆ3ï¼šStreamä¸Šä¼ ï¼ˆå®æ—¶å¤„ç†ï¼‰

**åŸç†**: ä½¿ç”¨ReadableStreamè¾¹è¯»è¾¹ä¸Šä¼ ï¼Œæ— éœ€ç­‰å¾…å®Œæ•´æ–‡ä»¶

```javascript
async function streamUpload(file) {
  const stream = file.stream();
  const response = await fetch('/api/upload/stream', {
    method: 'POST',
    body: stream,
    duplex: 'half',
    headers: {
      'Content-Type': 'application/octet-stream',
      'X-File-Name': file.name,
      'X-File-Size': file.size,
    }
  });
  return response;
}

// Workersç«¯
async function handleStreamUpload(request, env) {
  const fileName = request.headers.get('X-File-Name');
  const fileId = generateFileId();

  // ç›´æ¥æµå¼å†™å…¥R2
  await env.FILE_STORAGE.put(fileId, request.body);

  return jsonResponse({ success: true, fileId });
}
```

**ä¼˜åŠ¿**: å†…å­˜å ç”¨æœ€å°ï¼Œé€‚åˆè¶…å¤§æ–‡ä»¶

---

## ğŸ¯ æ¨èå®æ–½æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | å®ç°éš¾åº¦ | æ€§èƒ½æå‡ | é€‚ç”¨åœºæ™¯ | æ¨èåº¦ |
|------|---------|---------|---------|--------|
| **æ–¹æ¡ˆ1: å®¢æˆ·ç«¯åˆ†å—** | â­â­â­ ä¸­ç­‰ | â­â­â­â­â­ 300-500% | æ‰€æœ‰å¤§æ–‡ä»¶ | âœ… å¼ºçƒˆæ¨è |
| æ–¹æ¡ˆ2: R2 Multipart | â­â­â­â­ è¾ƒé«˜ | â­â­â­â­â­ 500%+ | è¶…å¤§æ–‡ä»¶(>1GB) | â­ å¯é€‰ |
| æ–¹æ¡ˆ3: Streamä¸Šä¼  | â­â­ ç®€å• | â­â­â­ 100-200% | è§†é¢‘æµåª’ä½“ | â­ ç‰¹å®šåœºæ™¯ |

## ğŸ“ˆ å…¶ä»–ä¼˜åŒ–æŠ€å·§

### 1. å‹ç¼©ä¼˜åŒ–

```javascript
// è°ƒæ•´å‹ç¼©çº§åˆ«ï¼ˆå·²å®æ–½ï¼‰
const zipped = zipSync(filesToZip, {
  level: 1,  // 0=æ— å‹ç¼©, 1=å¿«é€Ÿ, 6=é»˜è®¤, 9=æœ€å¤§
});
```

**æ•ˆæœ**:
- level 1: é€Ÿåº¦å¿«3-5å€ï¼Œå‹ç¼©ç‡é™ä½10-20%
- level 3: é€Ÿåº¦å¿«2å€ï¼Œå‹ç¼©ç‡é€‚ä¸­ï¼ˆå½“å‰ä½¿ç”¨ï¼‰

### 2. è¿æ¥ä¼˜åŒ–

åœ¨HTMLä¸­æ·»åŠ é¢„è¿æ¥ï¼š

```html
<head>
  <!-- é¢„è¿æ¥åˆ°WorkersåŸŸå -->
  <link rel="preconnect" href="https://your-worker.workers.dev">
  <link rel="dns-prefetch" href="https://your-worker.workers.dev">
</head>
```

### 3. å®¢æˆ·ç«¯ä¼˜åŒ–

```javascript
// ä½¿ç”¨Web Workerå¤„ç†æ–‡ä»¶
const worker = new Worker('file-processor.js');
worker.postMessage({ file, action: 'chunk' });

// ä½¿ç”¨IndexedDBç¼“å­˜åˆ†å—çŠ¶æ€ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
async function saveChunkStatus(uploadId, chunkIndex) {
  const db = await openDB('uploads');
  await db.put('chunks', { uploadId, chunkIndex, uploaded: true });
}
```

### 4. è¿›åº¦æ˜¾ç¤ºä¼˜åŒ–

```javascript
// ä½¿ç”¨requestAnimationFrameä¼˜åŒ–UIæ›´æ–°
let lastUpdate = 0;
function updateProgressThrottled(progress) {
  const now = Date.now();
  if (now - lastUpdate > 100) { // æœ€å¤š100msæ›´æ–°ä¸€æ¬¡
    updateProgress(progress);
    lastUpdate = now;
  }
}
```

## ğŸ”§ ç›‘æ§å’Œè°ƒè¯•

### æ€§èƒ½ç›‘æ§

```javascript
// æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ”¶é›†
performance.mark('upload-start');

// ... ä¸Šä¼ é€»è¾‘ ...

performance.mark('upload-end');
performance.measure('upload-duration', 'upload-start', 'upload-end');

const metrics = performance.getEntriesByName('upload-duration')[0];
console.log(`Upload took ${metrics.duration}ms`);

// å‘é€åˆ°åˆ†ææœåŠ¡
navigator.sendBeacon('/api/metrics', JSON.stringify({
  duration: metrics.duration,
  fileSize: file.size,
  throughput: file.size / (metrics.duration / 1000) // bytes/sec
}));
```

### Cloudflare Workers åˆ†æ

åœ¨wrangler.tomlä¸­å¯ç”¨ï¼š

```toml
[observability]
enabled = true
head_sampling_rate = 1
```

## ğŸ“Š é¢„æœŸæ€§èƒ½å¯¹æ¯”

### å½“å‰æ€§èƒ½ï¼ˆæ— ä¼˜åŒ–ï¼‰

| æ–‡ä»¶å¤§å° | ä¸Šä¼ æ—¶é—´ | ååé‡ |
|---------|---------|--------|
| 100MB | ~60ç§’ | ~1.7 MB/s |
| 1GB | ~10åˆ†é’Ÿ | ~1.7 MB/s |
| 10GB | ~100åˆ†é’Ÿ | ~1.7 MB/s |

### ä¼˜åŒ–åæ€§èƒ½ï¼ˆæ–¹æ¡ˆ1ï¼‰

| æ–‡ä»¶å¤§å° | ä¸Šä¼ æ—¶é—´ | ååé‡ | æå‡ |
|---------|---------|--------|------|
| 100MB | ~10ç§’ | ~10 MB/s | 6x â¬†ï¸ |
| 1GB | ~2åˆ†é’Ÿ | ~8.5 MB/s | 5x â¬†ï¸ |
| 10GB | ~20åˆ†é’Ÿ | ~8.5 MB/s | 5x â¬†ï¸ |

### æé™æ€§èƒ½ï¼ˆæ–¹æ¡ˆ2ï¼‰

| æ–‡ä»¶å¤§å° | ä¸Šä¼ æ—¶é—´ | ååé‡ | æå‡ |
|---------|---------|--------|------|
| 100MB | ~1ç§’ | ~100 MB/s | 60x â¬†ï¸ |
| 1GB | ~6ç§’ | ~170 MB/s | 100x â¬†ï¸ |
| 10GB | ~60ç§’ | ~170 MB/s | 100x â¬†ï¸ |

*æ³¨ï¼šå®é™…æ€§èƒ½å—ç”¨æˆ·ç½‘ç»œå¸¦å®½é™åˆ¶*

## ğŸš€ ç«‹å³å¼€å§‹

### å¿«é€Ÿå®æ–½ï¼ˆæ–¹æ¡ˆ1ï¼‰

1. **å‰ç«¯**: å¤åˆ¶å®¢æˆ·ç«¯åˆ†å—ä»£ç 
2. **åç«¯**: æ·»åŠ 3ä¸ªæ–°APIç«¯ç‚¹
3. **é…ç½®**: è°ƒæ•´CHUNK_SIZEå’ŒMAX_CONCURRENT
4. **æµ‹è¯•**: ä¸Šä¼ å¤§æ–‡ä»¶éªŒè¯

### éœ€è¦çš„ä¾èµ–

```json
{
  "dependencies": {
    "fflate": "^0.8.2"  // å·²å®‰è£…
  }
}
```

æ— éœ€é¢å¤–ä¾èµ–ï¼

## ğŸ“š å‚è€ƒæ–‡æ¡£

- [Cloudflare R2 Multipart Upload](https://developers.cloudflare.com/r2/objects/multipart-objects/)
- [Workers Upload Limits](https://developers.cloudflare.com/workers/platform/limits/)
- [R2 Performance Best Practices](https://developers.cloudflare.com/r2/objects/upload-objects/)

---

**å»ºè®®**: ä¼˜å…ˆå®æ–½**æ–¹æ¡ˆ1ï¼ˆå®¢æˆ·ç«¯åˆ†å—ä¸Šä¼ ï¼‰**ï¼Œå¯ä»¥è·å¾—æ˜¾è‘—çš„æ€§èƒ½æå‡ä¸”å®ç°ç›¸å¯¹ç®€å•ã€‚
