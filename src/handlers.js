/**
 * FastFile - ä¸šåŠ¡é€»è¾‘å¤„ç†å™¨
 * åŒ…å«æ‰€æœ‰è¯·æ±‚å¤„ç†å‡½æ•°å’Œæ–‡ä»¶æ“ä½œé€»è¾‘
 */

import { AwsClient } from 'aws4fetch';
import { Zip, ZipPassThrough } from 'fflate';
import {
  generateFileId,
  isValidPassword,
  getExpiryTime,
  isExpired,
  hashPassword,
  verifyPassword,
  jsonResponse,
  errorResponse
} from './utils.js';

// ç”¨äºå­˜å‚¨å‹ç¼©è¿›åº¦çš„ä¸´æ—¶çŠ¶æ€
export const compressionProgress = new Map();

/**
 * åˆå§‹åŒ–åˆ†å—ä¸Šä¼  (Phase 1)
 */
export async function handleUploadInit(request, env, logger, metrics, CONFIG, retryWithBackoff, getAwsClient, getR2Url, parseXmlResponse) {
  const requestLogger = logger ? logger.child({ handler: 'upload.init' }) : { info: () => { }, warn: () => { }, error: () => { } };

  try {
    const { files, password } = await request.json();

    requestLogger.info('Upload init request', {
      filesCount: files.length,
      totalSize: files.reduce((sum, f) => sum + f.size, 0)
    });

    // éªŒè¯å¯†ç 
    if (!password || !isValidPassword(password)) {
      requestLogger.warn('Invalid password provided');
      if (metrics) metrics.increment('upload.init.invalid_password', 1);
      return errorResponse('å¯†ç å¿…é¡»æ˜¯4ä½æ•°å­—');
    }

    // éªŒè¯æ–‡ä»¶
    if (!files || files.length === 0) {
      return errorResponse('è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶');
    }

    // ç”Ÿæˆä¸Šä¼ ID
    const uploadId = generateFileId();
    const hashedPwd = await hashPassword(password);

    // æ£€æŸ¥æ˜¯å¦ä¸ºå•ä¸ªzipæ–‡ä»¶ï¼ˆè·³è¿‡å‹ç¼©ï¼‰
    const isSingleZip = files.length === 1 && files[0].name.toLowerCase().endsWith('.zip');

    // åˆå§‹åŒ–aws4fetchå®¢æˆ·ç«¯
    const awsClient = getAwsClient(env);
    const r2Url = getR2Url(env);

    // ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºmultipart uploadå¹¶ç”Ÿæˆé¢„ç­¾å URL
    const fileUploads = [];
    for (const file of files) {
      const tempKey = `temp/${uploadId}/${file.name}`;
      const fileStartTime = Date.now();

      // ä½¿ç”¨é‡è¯•æœºåˆ¶åˆ›å»ºmultipart upload
      const xmlResult = await retryWithBackoff(
        async () => {
          const createResponse = await awsClient.fetch(`${r2Url}/${tempKey}?uploads`, {
            method: 'POST',
          });

          if (!createResponse.ok) {
            const errorText = await createResponse.text();
            const error = new Error(`åˆ›å»ºmultipart uploadå¤±è´¥: ${errorText}`);
            error.statusCode = createResponse.status;
            throw error;
          }

          return await parseXmlResponse(createResponse);
        },
        CONFIG.MAX_RETRY_ATTEMPTS,
        `Create multipart upload for ${file.name}`
      );

      const totalChunks = Math.ceil(file.size / CONFIG.CHUNK_SIZE);

      // ğŸ”§ æ–°å¢ï¼šä¸ºæ¯ä¸ª part ç”Ÿæˆé¢„ç­¾å URL
      const parts = [];
      const presignStartTime = Date.now();

      for (let partNumber = 1; partNumber <= totalChunks; partNumber++) {
        // ç”Ÿæˆç­¾åè¯·æ±‚ä¿¡æ¯
        const uploadUrl = `${r2Url}/${tempKey}?partNumber=${partNumber}&uploadId=${encodeURIComponent(xmlResult.UploadId)}`;

        const signedRequest = await awsClient.sign(uploadUrl, {
          method: 'PUT',
          headers: {
            'Content-Type': 'application/octet-stream'
          }
        });

        // æå–ç­¾åheadersï¼ˆåŒ…å«AWSç­¾åè®¤è¯ä¿¡æ¯ï¼‰
        const signedHeaders = {};
        signedRequest.headers.forEach((value, key) => {
          signedHeaders[key] = value;
        });

        parts.push({
          partNumber,
          uploadUrl: signedRequest.url,  // R2 endpoint URL
          headers: signedHeaders          // ğŸ”§ æ–°å¢ï¼šç­¾åheaders
        });
      }

      const presignDuration = Date.now() - presignStartTime;
      requestLogger.info('Generated presigned URLs', {
        fileName: file.name,
        totalChunks,
        presignDuration: `${presignDuration}ms`
      });

      fileUploads.push({
        name: file.name,
        size: file.size,
        key: tempKey,
        uploadId: xmlResult.UploadId,
        totalChunks,
        parts  // ğŸ”§ æ–°å¢ï¼šè¿”å›é¢„ç­¾å URL åˆ—è¡¨
      });

      const fileInitDuration = Date.now() - fileStartTime;
      requestLogger.info('File init completed', {
        fileName: file.name,
        totalDuration: `${fileInitDuration}ms`
      });
    }

    // ä¿å­˜ä¸Šä¼ å…ƒæ•°æ®
    const uploadMeta = {
      uploadId,
      password: hashedPwd,
      files: fileUploads,
      isSingleZip,
      totalSize: files.reduce((sum, f) => sum + f.size, 0),
      uploadedAt: Date.now(),
      status: 'uploading' // uploading, uploaded, compressing, completed, failed
    };

    await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(uploadMeta));

    requestLogger.info('Upload init completed', {
      uploadId,
      filesCount: files.length,
      totalChunks: fileUploads.reduce((sum, f) => sum + f.totalChunks, 0)
    });

    return jsonResponse({
      success: true,
      uploadId,
      files: fileUploads.map(f => ({
        name: f.name,
        totalChunks: f.totalChunks,
        uploadId: f.uploadId,
        parts: f.parts  // ğŸ”§ æ–°å¢ï¼šè¿”å›é¢„ç­¾å URL
      })),
      isSingleZip,
      chunkSize: CONFIG.CHUNK_SIZE
    });

  } catch (error) {
    console.error('Init error:', error);
    return errorResponse('åˆå§‹åŒ–å¤±è´¥: ' + error.message, 500);
  }
}

/**
 * ä¸Šä¼ å•ä¸ªåˆ†å—
 */
export async function handleUploadChunk(request, env, logger, metrics, CONFIG, retryWithBackoff, getAwsClient, getR2Url) {
  const requestLogger = logger ? logger.child({ handler: 'upload.chunk' }) : { info: () => { }, debug: () => { }, error: () => { } };
  const startTime = Date.now();

  try {
    const formData = await request.formData();
    const uploadId = formData.get('uploadId');
    const fileName = formData.get('fileName');
    const chunkIndex = parseInt(formData.get('chunkIndex'));
    const chunk = formData.get('chunk');

    // è·å–ä¸Šä¼ å…ƒæ•°æ®
    const metaStr = await env.FILE_META.get(`upload:${uploadId}`);
    if (!metaStr) {
      return errorResponse('ä¸Šä¼ ä¸å­˜åœ¨', 404);
    }

    const meta = JSON.parse(metaStr);
    const fileUpload = meta.files.find(f => f.name === fileName);

    if (!fileUpload) {
      return errorResponse('æ–‡ä»¶ä¸å­˜åœ¨', 404);
    }

    // åˆå§‹åŒ–aws4fetchå®¢æˆ·ç«¯
    const awsClient = getAwsClient(env);
    const r2Url = getR2Url(env);

    // ä½¿ç”¨aws4fetchä¸Šä¼ åˆ†å—
    const partNumber = chunkIndex + 1; // Part numberä»1å¼€å§‹
    const chunkBody = await chunk.arrayBuffer();

    // ä½¿ç”¨é‡è¯•æœºåˆ¶ä¸Šä¼ åˆ†å—
    const { response, etag } = await retryWithBackoff(
      async () => {
        const uploadResponse = await awsClient.fetch(
          `${r2Url}/${fileUpload.key}?partNumber=${partNumber}&uploadId=${fileUpload.uploadId}`,
          {
            method: 'PUT',
            body: chunkBody,
          }
        );

        if (!uploadResponse.ok) {
          const errorText = await uploadResponse.text();
          const error = new Error(`åˆ†å—ä¸Šä¼ å¤±è´¥: ${errorText}`);
          error.statusCode = uploadResponse.status;
          throw error;
        }

        const uploadEtag = uploadResponse.headers.get('etag');
        return { response: uploadResponse, etag: uploadEtag };
      },
      CONFIG.MAX_RETRY_ATTEMPTS,
      `Upload chunk ${partNumber} for ${fileName}`
    );

    // ğŸ”§ ä¿®å¤ç«æ€æ¡ä»¶ï¼šä¸ºæ¯ä¸ª chunk å•ç‹¬å­˜å‚¨ KV è®°å½•
    // é¿å…å¹¶å‘ä¿®æ”¹åŒä¸€ä¸ªå…ƒæ•°æ®å¯¹è±¡
    const chunkKey = `upload:${uploadId}:chunk:${fileName}:${chunkIndex}`;
    await env.FILE_META.put(chunkKey, JSON.stringify({
      partNumber,
      etag,
      fileName,
      chunkIndex,
      uploadedAt: Date.now()
    }));

    requestLogger.info('Chunk uploaded and recorded', {
      uploadId,
      fileName,
      chunkIndex,
      partNumber
    });

    // è®¡ç®—æ€»ä½“è¿›åº¦ï¼ˆä»ç‹¬ç«‹çš„ chunk è®°å½•ä¸­ç»Ÿè®¡ï¼‰
    const totalChunks = meta.files.reduce((sum, f) => sum + f.totalChunks, 0);
    let uploadedCount = 0;

    // ç»Ÿè®¡å·²ä¸Šä¼ çš„ chunks
    for (const file of meta.files) {
      for (let i = 0; i < file.totalChunks; i++) {
        const key = `upload:${uploadId}:chunk:${file.name}:${i}`;
        const exists = await env.FILE_META.get(key);
        if (exists) uploadedCount++;
      }
    }

    const progress = (uploadedCount / totalChunks) * 100;

    return jsonResponse({
      success: true,
      uploaded: uploadedCount,
      total: totalChunks,
      overallProgress: progress
    });

  } catch (error) {
    console.error('Chunk upload error:', error);
    return errorResponse('åˆ†å—ä¸Šä¼ å¤±è´¥: ' + error.message, 500);
  }
}

/**
 * ğŸ”§ æ–°å¢ï¼šç¡®è®¤ chunk ä¸Šä¼ ï¼ˆå‰ç«¯ç›´æ¥ä¸Šä¼ åˆ° R2 åè°ƒç”¨ï¼‰
 * æ­¤ç«¯ç‚¹éå¸¸è½»é‡çº§ï¼Œä¸å¤„ç†æ–‡ä»¶æ•°æ®ï¼Œåªè®°å½• ETag
 */
export async function handleUploadChunkConfirm(request, env, logger, metrics) {
  const requestLogger = logger ? logger.child({ handler: 'upload.chunk.confirm' }) : { info: () => { }, warn: () => { }, error: () => { } };
  const t0 = Date.now();

  try {
    const { uploadId, fileName, chunkIndex, partNumber, etag } = await request.json();

    const t1 = Date.now();
    console.log(`â±ï¸ [ChunkConfirm] Parse request: ${t1 - t0}ms`);

    // éªŒè¯å‚æ•°
    if (!uploadId || !fileName || chunkIndex === undefined || !partNumber || !etag) {
      requestLogger.warn('Missing required parameters', { uploadId, fileName, chunkIndex, partNumber, etag });
      return errorResponse('ç¼ºå°‘å¿…è¦å‚æ•°', 400);
    }

    // â­ å¹¶è¡Œè·å–ï¼šä¸Šä¼ å…ƒæ•°æ® + æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    const chunkKey = `upload:${uploadId}:chunk:${fileName}:${chunkIndex}`;

    const [metaStr, existingChunk] = await Promise.all([
      env.FILE_META.get(`upload:${uploadId}`),
      env.FILE_META.get(chunkKey)
    ]);

    if (!metaStr) {
      return errorResponse('ä¸Šä¼ ä¸å­˜åœ¨', 404);
    }

    const t2 = Date.now();
    console.log(`â±ï¸ [ChunkConfirm] Get meta (parallel): ${t2 - t1}ms`);

    const meta = JSON.parse(metaStr);
    const fileUpload = meta.files.find(f => f.name === fileName);

    if (!fileUpload) {
      return errorResponse('æ–‡ä»¶ä¸å­˜åœ¨', 404);
    }

    // â­ å‡†å¤‡å†™å…¥æ“ä½œ
    const chunkData = {
      partNumber,
      etag,
      fileName,
      chunkIndex,
      uploadedAt: Date.now()
    };

    // â­ æ›´æ–°è®¡æ•°å™¨ï¼ˆåªåœ¨æ–°å¢æ—¶ï¼‰
    const isNewChunk = !existingChunk;
    let uploadedCount = meta.uploadedCount || 0;
    let needsMetaUpdate = false;

    if (isNewChunk) {
      uploadedCount++;
      meta.uploadedCount = uploadedCount;
      needsMetaUpdate = true;
    }

    // â­ å¹¶è¡Œå†™å…¥ï¼šchunk æ•°æ® + å…ƒæ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    const writePromises = [
      env.FILE_META.put(chunkKey, JSON.stringify(chunkData))
    ];

    if (needsMetaUpdate) {
      writePromises.push(
        env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(meta))
      );
    }

    await Promise.all(writePromises);

    const t3 = Date.now();
    console.log(`â±ï¸ [ChunkConfirm] Save (parallel writes): ${t3 - t2}ms`);

    requestLogger.info('Chunk confirmed', {
      uploadId,
      fileName,
      chunkIndex,
      partNumber,
      isNewChunk,
      uploadedCount,
      etag: etag.substring(0, 10) + '...'
    });

    // â­ è®¡ç®—è¿›åº¦ - O(1) æ“ä½œ
    const totalChunks = meta.files.reduce((sum, f) => sum + f.totalChunks, 0);
    const progress = (uploadedCount / totalChunks) * 100;

    const totalDuration = Date.now() - t0;
    console.log(`â±ï¸ [ChunkConfirm] Total: ${totalDuration}ms (optimized, target: <100ms)`);

    if (metrics) {
      metrics.timing('chunk.confirm.duration', totalDuration);
      metrics.increment('chunk.confirm.success', 1);
      metrics.gauge('chunk.confirm.uploaded_count', uploadedCount);
    }

    return jsonResponse({
      success: true,
      uploaded: uploadedCount,
      total: totalChunks,
      overallProgress: progress.toFixed(2),
      isNewChunk  // å‘Šè¯‰å®¢æˆ·ç«¯æ˜¯å¦æ˜¯é‡å¤æäº¤
    });

  } catch (error) {
    console.error('âŒ [ChunkConfirm] Error:', error);
    if (logger) logger.error('Chunk confirm failed', { error: error.message });
    if (metrics) metrics.increment('chunk.confirm.error', 1);
    return errorResponse('ç¡®è®¤ä¸Šä¼ å¤±è´¥: ' + error.message, 500);
  }
}

/**
 * å®Œæˆä¸Šä¼ å¹¶è§¦å‘å‹ç¼©
 */
export async function handleUploadComplete(request, env, ctx, logger, metrics, CONFIG, retryWithBackoff, getAwsClient, getR2Url, performCompression) {
  const requestLogger = logger ? logger.child({ handler: 'upload.complete' }) : { info: () => { }, error: () => { } };

  try {
    const { uploadId } = await request.json();

    // è·å–ä¸Šä¼ å…ƒæ•°æ®
    const metaStr = await env.FILE_META.get(`upload:${uploadId}`);
    if (!metaStr) {
      return errorResponse('ä¸Šä¼ ä¸å­˜åœ¨', 404);
    }

    const meta = JSON.parse(metaStr);

    // ğŸ”§ ä»ç‹¬ç«‹çš„ chunk KV è®°å½•ä¸­è¯»å–æ‰€æœ‰ chunks
    // ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨ KV List API + å¹¶è¡Œè¯»å–ï¼Œå¤§å¹…æå‡æ€§èƒ½
    // æ€§èƒ½æå‡ï¼šå¯¹äº 1000 chunksï¼Œä» ~20ç§’ â†’ ~2ç§’ (10å€æå‡)
    const filesStatus = [];

    for (const fileUpload of meta.files) {
      const prefix = `upload:${uploadId}:chunk:${fileUpload.name}:`;

      // ğŸš€ ä½¿ç”¨ List API è·å–è¯¥æ–‡ä»¶çš„æ‰€æœ‰ chunk keys (å•æ¬¡è°ƒç”¨)
      const chunkList = await env.FILE_META.list({ prefix });

      requestLogger.info('Fetched chunk keys via List API', {
        fileName: fileUpload.name,
        keysFound: chunkList.keys.length,
        expectedChunks: fileUpload.totalChunks
      });

      // ğŸš€ å¹¶è¡Œè¯»å–æ‰€æœ‰ chunks
      const chunkPromises = chunkList.keys.map(async (key) => {
        const chunkDataStr = await env.FILE_META.get(key.name);
        if (chunkDataStr) {
          return JSON.parse(chunkDataStr);
        }
        return null;
      });

      const chunks = (await Promise.all(chunkPromises)).filter(c => c !== null);

      // æŒ‰ partNumber æ’åºï¼ˆç¡®ä¿é¡ºåºæ­£ç¡®ï¼‰
      chunks.sort((a, b) => a.partNumber - b.partNumber);

      filesStatus.push({
        name: fileUpload.name,
        uploadedChunks: chunks.length,
        totalChunks: fileUpload.totalChunks
      });

      // ä¿å­˜ chunks åˆ° fileUpload å¯¹è±¡ï¼Œç”¨äºåç»­å®Œæˆ multipart upload
      fileUpload.chunks = chunks;
    }

    requestLogger.info('Upload complete request', {
      uploadId,
      filesCount: meta.files.length,
      files: filesStatus
    });

    // éªŒè¯æ‰€æœ‰æ–‡ä»¶çš„æ‰€æœ‰åˆ†å—éƒ½å·²ä¸Šä¼ 
    for (const fileUpload of meta.files) {
      if (fileUpload.chunks.length !== fileUpload.totalChunks) {
        requestLogger.error('File incomplete', {
          fileName: fileUpload.name,
          uploadedChunks: fileUpload.chunks.length,
          totalChunks: fileUpload.totalChunks,
          missing: fileUpload.totalChunks - fileUpload.chunks.length
        });
        if (metrics) metrics.increment('upload.complete.incomplete', 1);
        return errorResponse(`æ–‡ä»¶ ${fileUpload.name} æœªå®Œå…¨ä¸Šä¼ : ${fileUpload.chunks.length}/${fileUpload.totalChunks} chunks`);
      }
    }

    requestLogger.info('All chunks verified, completing multipart upload');

    // åˆå§‹åŒ–aws4fetchå®¢æˆ·ç«¯
    const awsClient = getAwsClient(env);
    const r2Url = getR2Url(env);

    // å®Œæˆæ‰€æœ‰æ–‡ä»¶çš„multipart upload
    for (const fileUpload of meta.files) {
      // æŒ‰partNumberæ’åº
      const sortedParts = fileUpload.chunks.sort((a, b) => a.partNumber - b.partNumber);

      // è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰ parts ä¿¡æ¯
      console.log(`ğŸ“¦ [Complete] File: ${fileUpload.name}, Total parts: ${sortedParts.length}`);
      sortedParts.forEach(part => {
        console.log(`  Part ${part.partNumber}: ETag=${part.etag}, ChunkIndex=${part.chunkIndex}`);
      });

      // æ„å»ºXML body
      const partsXml = sortedParts
        .map(part => `<Part><PartNumber>${part.partNumber}</PartNumber><ETag>${part.etag}</ETag></Part>`)
        .join('');
      const xmlBody = `<CompleteMultipartUpload>${partsXml}</CompleteMultipartUpload>`;

      console.log(`ğŸ“ [Complete] XML Body length: ${xmlBody.length} bytes`);
      console.log(`ğŸ“ [Complete] First 500 chars: ${xmlBody.substring(0, 500)}`);

      // ä½¿ç”¨é‡è¯•æœºåˆ¶å®Œæˆmultipart upload
      await retryWithBackoff(
        async () => {
          const completeResponse = await awsClient.fetch(
            `${r2Url}/${fileUpload.key}?uploadId=${fileUpload.uploadId}`,
            {
              method: 'POST',
              body: xmlBody,
              headers: {
                'Content-Type': 'application/xml',
              },
            }
          );

          if (!completeResponse.ok) {
            const errorText = await completeResponse.text();
            console.error(`âŒ [Complete] Error response (${completeResponse.status}): ${errorText}`);
            const error = new Error(`å®Œæˆmultipart uploadå¤±è´¥: ${errorText}`);
            error.statusCode = completeResponse.status;
            throw error;
          }

          console.log(`âœ… [Complete] Multipart upload completed successfully for ${fileUpload.name}`);
          return completeResponse;
        },
        CONFIG.MAX_RETRY_ATTEMPTS,
        `Complete multipart upload for ${fileUpload.name}`
      );
    }

    // æ›´æ–°çŠ¶æ€
    meta.status = 'uploaded';
    meta.uploadedAt = Date.now();
    await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(meta));

    // å¦‚æœæ˜¯å•ä¸ªzipæ–‡ä»¶ï¼Œç›´æ¥ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®
    if (meta.isSingleZip) {
      const fileId = generateFileId();
      const fileUpload = meta.files[0];

      // å¤åˆ¶æ–‡ä»¶åˆ°æœ€ç»ˆä½ç½®
      await env.FILE_STORAGE.put(fileId, (await env.FILE_STORAGE.get(fileUpload.key)).body);

      // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
      await env.FILE_STORAGE.delete(fileUpload.key);

      // ä¿å­˜å…ƒæ•°æ®
      const metadata = {
        fileId,
        password: meta.password,
        expiryTime: getExpiryTime(),
        createdAt: Date.now(),
        fileName: fileUpload.name,
        fileSize: fileUpload.size,
      };

      await env.FILE_META.put(fileId, JSON.stringify(metadata));

      // æ›´æ–°ä¸Šä¼ çŠ¶æ€
      meta.status = 'completed';
      meta.fileId = fileId;
      await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(meta));

      return jsonResponse({
        success: true,
        status: 'completed',
        fileId,
        downloadUrl: `/d/${fileId}`
      });
    }

    // è§¦å‘å‹ç¼©ä»»åŠ¡
    console.log(`ğŸš€ [handleUploadComplete] Triggering compression task for uploadId: ${uploadId}`);
    const compressionPromise = performCompression(uploadId, meta, env);
    ctx.waitUntil(compressionPromise);
    console.log(`âœ… [handleUploadComplete] Compression task scheduled with ctx.waitUntil()`);

    return jsonResponse({
      success: true,
      status: 'compressing',
      message: 'å¼€å§‹å‹ç¼©'
    });

  } catch (error) {
    console.error('Complete error:', error);
    return errorResponse('å®Œæˆå¤±è´¥: ' + error.message, 500);
  }
}

/**
 * æŸ¥è¯¢ä¸Šä¼ çŠ¶æ€
 */
export async function handleUploadStatus(uploadId, env) {
  try {
    // å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„è¿›åº¦
    if (compressionProgress.has(uploadId)) {
      const progress = compressionProgress.get(uploadId);
      return jsonResponse({
        success: true,
        ...progress,
      });
    }

    // ä»KVä¸­æŸ¥è¯¢
    const uploadMetaStr = await env.FILE_META.get(`upload:${uploadId}`);
    if (!uploadMetaStr) {
      return errorResponse('ä¸Šä¼ ä¸å­˜åœ¨', 404);
    }

    const uploadMeta = JSON.parse(uploadMetaStr);

    if (uploadMeta.status === 'completed') {
      return jsonResponse({
        success: true,
        status: 'completed',
        progress: 100,
        fileId: uploadMeta.fileId,
        downloadUrl: `/d/${uploadMeta.fileId}`,
      });
    }

    if (uploadMeta.status === 'failed') {
      return jsonResponse({
        success: false,
        status: 'failed',
        error: uploadMeta.error || 'å¤„ç†å¤±è´¥',
      });
    }

    return jsonResponse({
      success: true,
      status: uploadMeta.status,
      progress: uploadMeta.status === 'compressing' ? 50 : 0,
    });

  } catch (error) {
    console.error('Status error:', error);
    return errorResponse('æŸ¥è¯¢çŠ¶æ€å¤±è´¥: ' + error.message, 500);
  }
}

/**
 * è·å–aws4fetchå®¢æˆ·ç«¯ (å†…éƒ¨è¾…åŠ©å‡½æ•°)
 */
function getAwsClient(env) {
  return new AwsClient({
    accessKeyId: env.R2_ACCESS_KEY_ID,
    secretAccessKey: env.R2_SECRET_ACCESS_KEY,
  });
}

/**
 * è·å–R2 bucket URL (å†…éƒ¨è¾…åŠ©å‡½æ•°)
 */
function getR2Url(env) {
  const accountId = env.R2_ACCOUNT_ID;
  const bucketName = env.R2_BUCKET_NAME || 'fastfile-storage';
  return `https://${accountId}.r2.cloudflarestorage.com/${bucketName}`;
}

/**
 * æ‰§è¡Œå®é™…çš„å‹ç¼©æ“ä½œ
 * ğŸ”§ ä½¿ç”¨æµå¼å‹ç¼©é¿å…å†…å­˜æº¢å‡º
 * ğŸ”§ æ™ºèƒ½ç¯å¢ƒæ£€æµ‹ï¼šç”Ÿäº§ç¯å¢ƒç”¨ R2 bindingï¼Œæœ¬åœ°å¼€å‘ç”¨ S3 API
 */
export async function performCompression(uploadId, uploadMeta, env) {
  console.log(`ğŸ”„ [Compression] Starting compression for uploadId: ${uploadId}`);

  try {
    uploadMeta.status = 'compressing';
    await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(uploadMeta));
    console.log(`âœ… [Compression] Status updated to 'compressing'`);

    // ğŸ¯ å•ä¸ª ZIP æ–‡ä»¶ä¼˜åŒ–ï¼šç›´æ¥å­˜å‚¨ä¸å‹ç¼©
    // å…¶ä»–æƒ…å†µï¼ˆå•ä¸ªé zip æ–‡ä»¶ã€å¤šä¸ªæ–‡ä»¶ï¼‰éƒ½éœ€è¦å‹ç¼©
    const isSingleZipFile = uploadMeta.files.length === 1 &&
      uploadMeta.files[0].name.toLowerCase().endsWith('.zip');

    if (isSingleZipFile) {
      console.log(`ğŸ“¦ [Compression] Single ZIP file detected, skipping compression`);
      return await handleSingleFile(uploadId, uploadMeta, env);
    }

    // å…¶ä»–æƒ…å†µï¼šä½¿ç”¨æµå¼å‹ç¼©
    if (uploadMeta.files.length === 1) {
      console.log(`ğŸ“„ [Compression] Single non-ZIP file detected, compressing...`);
    } else {
      console.log(`ğŸ“ [Compression] Multiple files detected, compressing...`);
    }
    return await handleMultipleFiles(uploadId, uploadMeta, env);

  } catch (error) {
    console.error(`âŒ [Compression] Error:`, error);
    uploadMeta.status = 'failed';
    uploadMeta.error = error.message;
    await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(uploadMeta));
    compressionProgress.delete(uploadId);
    throw error;
  }
}

/**
 * å¤„ç†å•æ–‡ä»¶ä¸Šä¼ ï¼ˆç›´æ¥å­˜å‚¨ä¸å‹ç¼©ï¼‰
 */
async function handleSingleFile(uploadId, uploadMeta, env) {
  const fileInfo = uploadMeta.files[0];
  console.log(`ğŸ“‚ [SingleFile] Processing: ${fileInfo.name}`);

  let useS3API = false;
  const awsClient = getAwsClient(env);
  const r2Url = getR2Url(env);

  // è¯»å–æ–‡ä»¶
  let fileData;
  try {
    const r2Object = await env.FILE_STORAGE.get(fileInfo.key);
    if (r2Object) {
      fileData = await r2Object.arrayBuffer();
      console.log(`âœ… [SingleFile] Read via R2 binding: ${fileData.byteLength} bytes`);
    } else {
      useS3API = true;
    }
  } catch (error) {
    console.log(`âš ï¸ [SingleFile] R2 binding failed, using S3 API`);
    useS3API = true;
  }

  if (useS3API || !fileData) {
    const response = await awsClient.fetch(`${r2Url}/${fileInfo.key}`);
    if (!response.ok) {
      throw new Error(`æ–‡ä»¶ä¸å­˜åœ¨: ${fileInfo.name}`);
    }
    fileData = await response.arrayBuffer();
    console.log(`âœ… [SingleFile] Read via S3 API: ${fileData.byteLength} bytes`);
  }

  // ç”Ÿæˆæ–‡ä»¶IDå¹¶å­˜å‚¨
  const fileId = generateFileId();
  const expiryTime = getExpiryTime();

  await env.FILE_STORAGE.put(fileId, fileData);
  console.log(`âœ… [SingleFile] Saved with ID: ${fileId}`);

  // ä¿å­˜å…ƒæ•°æ®
  const metadata = {
    fileId,
    fileName: fileInfo.name,
    password: uploadMeta.password,
    expiryTime,
    uploadedAt: Date.now(),
    fileCount: 1,
    fileSize: fileData.byteLength,
  };

  await env.FILE_META.put(fileId, JSON.stringify(metadata));

  // æ›´æ–°ä¸Šä¼ çŠ¶æ€
  uploadMeta.status = 'completed';
  uploadMeta.fileId = fileId;
  await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(uploadMeta));

  // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
  await env.FILE_STORAGE.delete(fileInfo.key);

  compressionProgress.delete(uploadId);
  console.log(`ğŸ‰ [SingleFile] Completed: ${fileId}`);

  return fileId;
}

/**
 * å¤„ç†å¤šæ–‡ä»¶ï¼šGBçº§åˆ«æµå¼å‹ç¼©
 * - åˆ†å—è¯»å–ï¼šæ¯æ¬¡è¯»å–10MBé¿å…å†…å­˜æº¢å‡º
 * - æµå¼å‹ç¼©ï¼šä½¿ç”¨ fflate Zip æµå¼ API
 * - åˆ†å—å†™å…¥ï¼šä½¿ç”¨ R2 Multipart Upload è¾¹ç”Ÿæˆè¾¹ä¸Šä¼ 
 */
async function handleMultipleFiles(uploadId, uploadMeta, env) {
  console.log(`ğŸ—œï¸ [MultiFile] Starting GB-scale streaming compression for ${uploadMeta.files.length} files`);

  const CHUNK_READ_SIZE = 10 * 1024 * 1024; // 10MB åˆ†å—è¯»å–
  const awsClient = getAwsClient(env);
  const r2Url = getR2Url(env);

  // ğŸ¯ å‡†å¤‡ R2 Multipart Upload ç”¨äºå†™å…¥æœ€ç»ˆ ZIP
  const fileId = generateFileId();
  const expiryTime = getExpiryTime();

  console.log(`ğŸš€ [MultiFile] Initializing R2 Multipart Upload for final ZIP: ${fileId}`);
  const uploadIdForZip = await initMultipartUpload(fileId, awsClient, r2Url);

  const uploadedParts = [];
  let currentChunkBuffer = [];
  let currentChunkSize = 0;
  let partNumber = 1;

  // ğŸ”§ R2 ä¸¥æ ¼è¦æ±‚ï¼šé™¤æœ€åä¸€ä¸ª part å¤–ï¼Œæ‰€æœ‰ parts å¿…é¡»å¤§å°å®Œå…¨ç›¸åŒ
  const STANDARD_PART_SIZE = 50 * 1024 * 1024; // 50MB - æ ‡å‡† part å¤§å°
  const MIN_PART_SIZE = 5 * 1024 * 1024; // 5MB - R2 æœ€å°è¦æ±‚ï¼ˆä»…ç”¨äºæœ€åä¸€ä¸ª partï¼‰

  // ğŸ¯ åˆ›å»ºæµå¼ ZIP ç”Ÿæˆå™¨ï¼ˆè¾¹ç”Ÿæˆè¾¹ä¸Šä¼ åˆ° R2ï¼‰
  let zipError = null;
  let zipFinalized = false;
  let pendingUploads = [];  // ğŸ”§ æ”¶é›†æ‰€æœ‰å¾…å¤„ç†çš„ä¸Šä¼  Promise

  const zipStream = new Zip((err, chunk, final) => {
    if (err) {
      console.error(`âŒ [MultiFile] ZIP stream error:`, err);
      zipError = err;
      return;
    }

    if (chunk && chunk.byteLength > 0) {
      console.log(`ğŸ“¦ [MultiFile] ZIP chunk generated: ${chunk.byteLength} bytes`);

      // ç´¯ç§¯ chunk åˆ°ç¼“å†²åŒº
      currentChunkBuffer.push(chunk);
      currentChunkSize += chunk.byteLength;

      // ğŸ”§ å½“ç¼“å†²åŒº >= STANDARD_PART_SIZE æ—¶ï¼Œä¸Šä¼ ç²¾ç¡®å¤§å°çš„ part
      // è¿™ç¡®ä¿æ‰€æœ‰éæœ€åä¸€ä¸ª part çš„å¤§å°å®Œå…¨ç›¸åŒ
      while (currentChunkSize >= STANDARD_PART_SIZE) {
        // åˆå¹¶æ‰€æœ‰ chunks
        const allData = mergeUint8Arrays(currentChunkBuffer);

        // å–å‡ºç²¾ç¡®çš„ STANDARD_PART_SIZE
        const partData = allData.slice(0, STANDARD_PART_SIZE);
        const remainingData = allData.slice(STANDARD_PART_SIZE);

        const currentPartNumber = partNumber++;
        console.log(`â¬†ï¸ [MultiFile] Uploading part ${currentPartNumber}: ${partData.byteLength} bytes (standard size)`);

        // ğŸ”§ åˆ›å»ºä¸Šä¼  Promise å¹¶æ”¶é›†èµ·æ¥
        const uploadPromise = (async () => {
          try {
            const etag = await uploadPart(fileId, uploadIdForZip, currentPartNumber, partData, awsClient, r2Url);
            uploadedParts.push({ PartNumber: currentPartNumber, ETag: etag, Size: partData.byteLength });
          } catch (error) {
            console.error(`âŒ [MultiFile] Failed to upload part ${currentPartNumber}:`, error);
            zipError = error;
          }
        })();
        pendingUploads.push(uploadPromise);

        // å‰©ä½™æ•°æ®æ”¾å›ç¼“å†²åŒº
        if (remainingData.byteLength > 0) {
          currentChunkBuffer = [remainingData];
          currentChunkSize = remainingData.byteLength;
        } else {
          currentChunkBuffer = [];
          currentChunkSize = 0;
        }
      }
    }

    if (final) {
      console.log(`âœ… [MultiFile] ZIP stream finalized`);

      // ä¸Šä¼ æœ€åçš„ç¼“å†²åŒºï¼ˆå¦‚æœæœ‰ï¼‰
      // æœ€åä¸€ä¸ª part å¯ä»¥å°äº STANDARD_PART_SIZE
      if (currentChunkSize > 0) {
        const partData = mergeUint8Arrays(currentChunkBuffer);
        const currentPartNumber = partNumber++;
        console.log(`â¬†ï¸ [MultiFile] Uploading final part ${currentPartNumber}: ${partData.byteLength} bytes`);

        // ğŸ”§ åˆ›å»ºä¸Šä¼  Promise å¹¶æ”¶é›†èµ·æ¥
        const uploadPromise = (async () => {
          try {
            const etag = await uploadPart(fileId, uploadIdForZip, currentPartNumber, partData, awsClient, r2Url);
            uploadedParts.push({ PartNumber: currentPartNumber, ETag: etag, Size: partData.byteLength });
          } catch (error) {
            console.error(`âŒ [MultiFile] Failed to upload final part ${currentPartNumber}:`, error);
            zipError = error;
          }
        })();
        pendingUploads.push(uploadPromise);
      }

      // ğŸ”§ ç­‰å¾…æ‰€æœ‰ä¸Šä¼ å®Œæˆåå†è®¾ç½® zipFinalized = true
      Promise.all(pendingUploads)
        .then(() => {
          console.log(`âœ… [MultiFile] All ${pendingUploads.length} parts uploaded successfully`);
          zipFinalized = true;
        })
        .catch((error) => {
          console.error(`âŒ [MultiFile] Failed to upload parts:`, error);
          zipError = error;
          zipFinalized = true;  // å³ä½¿å¤±è´¥ä¹Ÿè¦è®¾ç½®ï¼Œä»¥ä¾¿å¤–å±‚æ£€æµ‹åˆ°é”™è¯¯
        });
    }
  });

  // ğŸ”„ é€ä¸ªæ–‡ä»¶åˆ†å—è¯»å–å¹¶æµå¼å‹ç¼©
  let processedCount = 0;

  for (const fileInfo of uploadMeta.files) {
    console.log(`ğŸ” [MultiFile] Processing file ${processedCount + 1}/${uploadMeta.files.length}: ${fileInfo.name}`);

    compressionProgress.set(uploadId, {
      status: 'reading',
      progress: Math.round((processedCount / uploadMeta.files.length) * 80),
      currentFile: fileInfo.name,
      processedCount,
      totalCount: uploadMeta.files.length,
    });

    // ğŸ—œï¸ åˆ›å»ºæ–‡ä»¶æµï¼ˆä¸å‹ç¼©ï¼Œlevel=0ï¼‰
    const fileStream = new ZipPassThrough(fileInfo.name);
    zipStream.add(fileStream);

    // ğŸ“– è·å–æ–‡ä»¶å¤§å°
    let fileSize;
    try {
      const headResponse = await awsClient.fetch(`${r2Url}/${fileInfo.key}`, { method: 'HEAD' });
      fileSize = parseInt(headResponse.headers.get('content-length') || '0');
      console.log(`ğŸ“ [MultiFile] File size: ${fileSize} bytes`);
    } catch (error) {
      console.warn(`âš ï¸ [MultiFile] Failed to get file size, will read in one go`);
      fileSize = null;
    }

    // ğŸ”„ åˆ†å—è¯»å–æ–‡ä»¶å¹¶æ¨é€åˆ°å‹ç¼©æµ
    if (fileSize && fileSize > CHUNK_READ_SIZE) {
      // å¤§æ–‡ä»¶ï¼šåˆ†å—è¯»å–
      let offset = 0;
      while (offset < fileSize) {
        const end = Math.min(offset + CHUNK_READ_SIZE - 1, fileSize - 1);
        console.log(`ğŸ“– [MultiFile] Reading chunk: bytes ${offset}-${end}`);

        const response = await awsClient.fetch(`${r2Url}/${fileInfo.key}`, {
          headers: { Range: `bytes=${offset}-${end}` },
        });

        if (!response.ok) {
          throw new Error(`Failed to read file chunk: ${response.status}`);
        }

        const chunkData = new Uint8Array(await response.arrayBuffer());
        const isFinal = (end >= fileSize - 1);

        fileStream.push(chunkData, isFinal);
        console.log(`âœ… [MultiFile] Pushed ${chunkData.byteLength} bytes to ZIP stream (final: ${isFinal})`);

        offset = end + 1;
      }
    } else {
      // å°æ–‡ä»¶ï¼šä¸€æ¬¡è¯»å–
      const response = await awsClient.fetch(`${r2Url}/${fileInfo.key}`);
      if (!response.ok) {
        throw new Error(`æ–‡ä»¶ä¸å­˜åœ¨: ${fileInfo.name}`);
      }

      const fileData = new Uint8Array(await response.arrayBuffer());
      fileStream.push(fileData, true);
      console.log(`âœ… [MultiFile] Pushed entire file (${fileData.byteLength} bytes) to ZIP stream`);
    }

    // ğŸ—‘ï¸ ç«‹å³åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    try {
      await env.FILE_STORAGE.delete(fileInfo.key);
      console.log(`ğŸ—‘ï¸ [MultiFile] Deleted temp file: ${fileInfo.key}`);
    } catch (error) {
      console.warn(`âš ï¸ [MultiFile] Failed to delete temp file: ${fileInfo.key}`);
    }

    processedCount++;

    if (zipError) {
      throw new Error(`ZIP stream error: ${zipError.message}`);
    }
  }

  console.log(`â„¹ï¸ [MultiFile] All ${uploadMeta.files.length} files added to ZIP stream`);

  // ğŸ ç»“æŸ ZIP æµ
  zipStream.end();
  console.log(`ğŸ [MultiFile] ZIP stream ended, waiting for finalization...`);

  // â³ ç­‰å¾… ZIP æµå®Œæˆ
  const maxWait = 60000; // 60ç§’è¶…æ—¶
  const startTime = Date.now();
  while (!zipFinalized && !zipError && (Date.now() - startTime) < maxWait) {
    await new Promise(resolve => setTimeout(resolve, 200));
  }

  if (zipError) {
    await abortMultipartUpload(fileId, uploadIdForZip, awsClient, r2Url);
    throw new Error(`å‹ç¼©å¤±è´¥: ${zipError.message || zipError}`);
  }

  if (!zipFinalized) {
    await abortMultipartUpload(fileId, uploadIdForZip, awsClient, r2Url);
    throw new Error('å‹ç¼©è¶…æ—¶');
  }

  // âœ… å®Œæˆ R2 Multipart Upload
  compressionProgress.set(uploadId, {
    status: 'finalizing',
    progress: 90,
    message: 'æ­£åœ¨å®Œæˆä¸Šä¼ ...',
  });

  console.log(`ğŸ [MultiFile] Completing R2 Multipart Upload with ${uploadedParts.length} parts`);
  await completeMultipartUpload(fileId, uploadIdForZip, uploadedParts, awsClient, r2Url);
  console.log(`âœ… [MultiFile] R2 Multipart Upload completed: ${fileId}`);

  // ğŸ’¾ ä¿å­˜å…ƒæ•°æ®
  const totalSize = uploadedParts.reduce((sum, part) => sum + part.Size || 0, 0);
  const metadata = {
    fileId,
    fileName: 'files.zip',
    password: uploadMeta.password,
    expiryTime,
    uploadedAt: Date.now(),
    fileCount: uploadMeta.files.length,
    fileSize: totalSize,
  };

  await env.FILE_META.put(fileId, JSON.stringify(metadata));

  // âœ… æ›´æ–°ä¸Šä¼ çŠ¶æ€
  uploadMeta.status = 'completed';
  uploadMeta.fileId = fileId;
  await env.FILE_META.put(`upload:${uploadId}`, JSON.stringify(uploadMeta));

  compressionProgress.delete(uploadId);
  console.log(`ğŸ‰ [MultiFile] Completed: ${fileId}`);

  return fileId;
}

/**
 * åˆå¹¶å¤šä¸ª Uint8Array
 */
function mergeUint8Arrays(arrays) {
  const totalLength = arrays.reduce((sum, arr) => sum + arr.byteLength, 0);
  const result = new Uint8Array(totalLength);
  let offset = 0;
  for (const arr of arrays) {
    result.set(arr, offset);
    offset += arr.byteLength;
  }
  return result;
}

/**
 * åˆå§‹åŒ– R2 Multipart Upload
 */
async function initMultipartUpload(key, awsClient, r2Url) {
  const url = `${r2Url}/${key}?uploads`;

  const response = await awsClient.fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/octet-stream',
    },
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Failed to init multipart upload: ${response.status} ${error}`);
  }

  const xmlText = await response.text();
  const uploadIdMatch = xmlText.match(/<UploadId>([^<]+)<\/UploadId>/);

  if (!uploadIdMatch) {
    throw new Error('Failed to extract UploadId from response');
  }

  const uploadId = uploadIdMatch[1];
  console.log(`ğŸš€ [Multipart] Initialized upload: ${uploadId}`);

  return uploadId;
}

/**
 * ä¸Šä¼ å•ä¸ª part
 */
async function uploadPart(key, uploadId, partNumber, data, awsClient, r2Url) {
  const url = `${r2Url}/${key}?partNumber=${partNumber}&uploadId=${encodeURIComponent(uploadId)}`;

  const response = await awsClient.fetch(url, {
    method: 'PUT',
    body: data,
    headers: {
      'Content-Type': 'application/octet-stream',
      'Content-Length': data.byteLength.toString(),
    },
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Failed to upload part ${partNumber}: ${response.status} ${error}`);
  }

  const etag = response.headers.get('etag');
  if (!etag) {
    throw new Error(`No ETag returned for part ${partNumber}`);
  }

  console.log(`âœ… [Multipart] Uploaded part ${partNumber}: ${etag}`);
  return etag;
}

/**
 * å®Œæˆ R2 Multipart Upload
 */
async function completeMultipartUpload(key, uploadId, parts, awsClient, r2Url) {
  const url = `${r2Url}/${key}?uploadId=${encodeURIComponent(uploadId)}`;

  // æ„é€  XML body
  const xmlParts = parts.map(part =>
    `<Part><PartNumber>${part.PartNumber}</PartNumber><ETag>${part.ETag}</ETag></Part>`
  ).join('');

  const xmlBody = `<CompleteMultipartUpload>${xmlParts}</CompleteMultipartUpload>`;

  const response = await awsClient.fetch(url, {
    method: 'POST',
    body: xmlBody,
    headers: {
      'Content-Type': 'application/xml',
    },
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Failed to complete multipart upload: ${response.status} ${error}`);
  }

  console.log(`ğŸ‰ [Multipart] Completed upload: ${key}`);
  return await response.text();
}

/**
 * ä¸­æ­¢ R2 Multipart Upload
 */
async function abortMultipartUpload(key, uploadId, awsClient, r2Url) {
  const url = `${r2Url}/${key}?uploadId=${encodeURIComponent(uploadId)}`;

  const response = await awsClient.fetch(url, {
    method: 'DELETE',
  });

  if (!response.ok) {
    const error = await response.text();
    console.error(`âš ï¸ [Multipart] Failed to abort upload: ${response.status} ${error}`);
  } else {
    console.log(`ğŸ—‘ï¸ [Multipart] Aborted upload: ${uploadId}`);
  }
}

/**
 * éªŒè¯å¯†ç 
 */
export async function handleVerify(request, env) {
  try {
    const { fileId, password } = await request.json();

    if (!fileId || !password) {
      return errorResponse('ç¼ºå°‘æ–‡ä»¶IDæˆ–å¯†ç ');
    }

    // è·å–æ–‡ä»¶å…ƒæ•°æ®
    const metadataStr = await env.FILE_META.get(fileId);

    if (!metadataStr) {
      return errorResponse('æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ', 404);
    }

    const metadata = JSON.parse(metadataStr);

    // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    if (isExpired(metadata.expiryTime)) {
      // åˆ é™¤è¿‡æœŸæ–‡ä»¶
      await deleteFile(fileId, env);
      return errorResponse('æ–‡ä»¶å·²è¿‡æœŸ', 410);
    }

    // éªŒè¯å¯†ç 
    const isValid = await verifyPassword(password, metadata.password);

    if (!isValid) {
      return errorResponse('å¯†ç é”™è¯¯', 401);
    }

    // ä½¿ç”¨å“ˆå¸Œåçš„å¯†ç ç”Ÿæˆä»¤ç‰Œ
    const downloadToken = await generateDownloadToken(fileId, metadata.password);

    return jsonResponse({
      success: true,
      fileId,
      fileName: metadata.fileName,
      fileSize: metadata.fileSize,
      downloadUrl: `/api/download/${fileId}?token=${downloadToken}`,
    });

  } catch (error) {
    console.error('Verify error:', error);
    return errorResponse('éªŒè¯å¤±è´¥: ' + error.message, 500);
  }
}

/**
 * å¤„ç†æ–‡ä»¶ä¸‹è½½
 */
export async function handleDownload(fileId, request, env) {
  try {
    const url = new URL(request.url);
    const token = url.searchParams.get('token');

    if (!token) {
      return errorResponse('ç¼ºå°‘ä¸‹è½½ä»¤ç‰Œ', 401);
    }

    // è·å–æ–‡ä»¶å…ƒæ•°æ®
    const metadataStr = await env.FILE_META.get(fileId);

    if (!metadataStr) {
      return errorResponse('æ–‡ä»¶ä¸å­˜åœ¨', 404);
    }

    const metadata = JSON.parse(metadataStr);

    // æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
    if (isExpired(metadata.expiryTime)) {
      await deleteFile(fileId, env);
      return errorResponse('æ–‡ä»¶å·²è¿‡æœŸ', 410);
    }

    // éªŒè¯ä»¤ç‰Œ
    const expectedToken = await generateDownloadToken(fileId, metadata.password);
    if (token !== expectedToken) {
      return errorResponse('æ— æ•ˆçš„ä¸‹è½½ä»¤ç‰Œ', 401);
    }

    // ğŸ”§ ä»R2è·å–æ–‡ä»¶ï¼ˆæ™ºèƒ½é€‰æ‹©è®¿é—®æ–¹å¼ï¼‰
    // ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨åŸç”Ÿ R2 Bindingï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ aws4fetchï¼ˆæœ¬åœ°å¼€å‘ç¯å¢ƒï¼‰

    console.log(`ğŸ“¥ [Download] Attempting to fetch file: ${fileId}`);

    // æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨åŸç”Ÿ R2 Bindingï¼ˆç”Ÿäº§ç¯å¢ƒæœ€ä¼˜ï¼‰
    try {
      const object = await env.FILE_STORAGE.get(fileId);

      if (object) {
        console.log(`âœ… [Download] File fetched via R2 Binding (production mode)`);
        return new Response(object.body, {
          headers: {
            'Content-Type': 'application/zip',
            'Content-Disposition': `attachment; filename="${encodeURIComponent(metadata.fileName)}"`,
            'Content-Length': metadata.fileSize.toString(),
            'Access-Control-Allow-Origin': '*',
          },
        });
      }

      console.log(`âš ï¸ [Download] R2 Binding returned null, trying aws4fetch...`);
    } catch (error) {
      console.log(`âš ï¸ [Download] R2 Binding failed: ${error.message}, trying aws4fetch...`);
    }

    // æ–¹æ¡ˆ2ï¼šä½¿ç”¨ aws4fetch è®¿é—®è¿œç¨‹ R2ï¼ˆæœ¬åœ°å¼€å‘ç¯å¢ƒ fallbackï¼‰
    try {
      const awsClient = getAwsClient(env);
      const r2Url = getR2Url(env);
      const downloadUrl = `${r2Url}/${fileId}`;

      console.log(`ğŸ”„ [Download] Fetching via aws4fetch (dev mode): ${downloadUrl}`);

      const response = await awsClient.fetch(downloadUrl, {
        method: 'GET',
      });

      if (response.ok) {
        console.log(`âœ… [Download] File fetched via aws4fetch`);
        return new Response(response.body, {
          headers: {
            'Content-Type': 'application/zip',
            'Content-Disposition': `attachment; filename="${encodeURIComponent(metadata.fileName)}"`,
            'Content-Length': metadata.fileSize.toString(),
            'Access-Control-Allow-Origin': '*',
          },
        });
      }

      console.error(`âŒ [Download] aws4fetch failed: ${response.status} ${response.statusText}`);
    } catch (error) {
      console.error(`âŒ [Download] aws4fetch error: ${error.message}`);
    }

    // ä¸¤ç§æ–¹å¼éƒ½å¤±è´¥äº†
    return errorResponse('æ–‡ä»¶æ•°æ®ä¸å­˜åœ¨', 404);

  } catch (error) {
    console.error('Download error:', error);
    return errorResponse('ä¸‹è½½å¤±è´¥: ' + error.message, 500);
  }
}

/**
 * ç”Ÿæˆä¸‹è½½ä»¤ç‰Œ
 */
async function generateDownloadToken(fileId, hashedPassword) {
  const data = `${fileId}:${hashedPassword}`;
  const encoder = new TextEncoder();
  const dataBuffer = encoder.encode(data);
  const hashBuffer = await crypto.subtle.digest('SHA-256', dataBuffer);
  const hashArray = Array.from(new Uint8Array(hashBuffer));
  const hashHex = hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  return hashHex;
}

/**
 * åˆ é™¤æ–‡ä»¶å’Œå…ƒæ•°æ®
 */
async function deleteFile(fileId, env) {
  try {
    await env.FILE_STORAGE.delete(fileId);
    await env.FILE_META.delete(fileId);
  } catch (error) {
    console.error('Delete error:', error);
  }
}

/**
 * æ¸…ç†è¿‡æœŸæ–‡ä»¶ï¼ˆå®šæ—¶ä»»åŠ¡ï¼‰
 */
export async function cleanupExpiredFiles(env) {
  try {
    const list = await env.FILE_META.list();
    let deletedCount = 0;

    for (const key of list.keys) {
      // è·³è¿‡ä¸Šä¼ å…ƒæ•°æ®
      if (key.name.startsWith('upload:')) {
        continue;
      }

      const metadataStr = await env.FILE_META.get(key.name);
      if (!metadataStr) continue;

      const metadata = JSON.parse(metadataStr);

      if (isExpired(metadata.expiryTime)) {
        await deleteFile(key.name, env);
        deletedCount++;
      }
    }

    console.log(`Cleaned up ${deletedCount} expired files`);
  } catch (error) {
    console.error('Cleanup error:', error);
  }
}
